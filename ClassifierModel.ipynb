{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweeter Tweets Classification Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add ability of llama3 to classify tweets\n",
    "- Use word2vec next time for word embeddings instead of tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import sklearn\n",
    "import warnings\n",
    "import re\n",
    "from IPython.display import display\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying tweets\n",
    "\n",
    "In this project, I will be analyzing Twitter data extracted using [this](https://dev.twitter.com/overview/api) api. The data contains tweets posted by the following six Twitter accounts: `realDonaldTrump, mike_pence, GOP, HillaryClinton, timkaine, TheDemocrats`\n",
    "\n",
    "For every tweet, there are two pieces of information:\n",
    "- `screen_name`: the Twitter handle of the user tweeting and\n",
    "- `text`: the content of the tweet.\n",
    "\n",
    "The tweets have been divided into two parts - train and test available to you in CSV files. For train, both the `screen_name` and `text` attributes were provided but for test, `screen_name` is hidden.\n",
    "\n",
    "The overarching goal of the problem is to \"predict\" the political inclination (Republican/Democratic) of the Twitter user from one of his/her tweets. The ground truth (i.e., true class labels) is determined from the `screen_name` of the tweet as follows\n",
    "- `realDonaldTrump, mike_pence, GOP` are Republicans\n",
    "- `HillaryClinton, timkaine, TheDemocrats` are Democrats\n",
    "\n",
    "Thus, this is a binary classification problem. \n",
    "\n",
    "The problem proceeds in 3 stages. The three stages are text processing, feature construction and tweet classification using a SVM classifier.\n",
    "- **Text processing**: We will clean up the raw tweet text using the various functions offered by the [nltk](http://www.nltk.org/genindex.html) package.\n",
    "- **Feature construction**: In this part, we will construct bag-of-words feature vectors and training labels from the processed text of tweets and the `screen_name` columns respectively.\n",
    "- **Classification**: Using the features derived, we will use [sklearn](http://scikit-learn.org/stable/modules/classes.html) package to learn a model which classifies the tweets as desired.\n",
    "\n",
    "\n",
    "I will be using the python packages in this problem: `nltk` and `sklearn`, both of which should be available with anaconda. However, NLTK comes with many corpora, toy grammars, trained models, etc, which have to be downloaded manually. This project requires NLTK's stopwords list, POS tagger, and WordNetLemmatizer. I install them using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('average_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Processing [20%]\n",
    "\n",
    "Build a function which processes and tokenizes raw text. The generated list of tokens should meet the following specifications:\n",
    "1. The tokens must all be in lower case.\n",
    "2. The tokens should appear in the same order as in the raw text.\n",
    "3. The tokens must be in their lemmatized form. If a word cannot be lemmatized (i.e, you get an exception), simply catch it and ignore it. These words will not appear in the token list.\n",
    "4. The tokens must not contain any punctuations. Punctuations should be handled as follows: (a) Apostrophe of the form `'s` must be ignored. e.g., `She's` becomes `she`. (b) Other apostrophes should be omitted. e.g, `don't` becomes `dont`. (c) Words must be broken at the hyphen and other punctuations. \n",
    "5. The tokens must not contain any part of a url.\n",
    "\n",
    "In order for `lemmatize()` to give me the root form for any word, I have to provide the context in which you want to lemmatize through the `pos` parameter: `lemmatizer.lemmatize(word, pos=SOMEVALUE)`. The context should be the part of speech (POS) for that word. I don't need to  manually write out the lexical categories for each word because [nltk.pos_tag()](https://www.nltk.org/book/ch05.html) will do this for me. I will then use the results from `pos_tag()` for the `pos` parameter.\n",
    "However, a thing that i noticed is that  the POS tag returned from `pos_tag()` is in different format than the expected pos by `lemmatizer`.\n",
    "> pos\n",
    "(Syntactic category): n for noun files, v for verb files, a for adjective files, r for adverb files.\n",
    "\n",
    "I will also need to map these pos appropriately. After searching `nltk.help.upenn_tagset()` provides description of each tag returned by `pos_tag()`. This will be helpful\n",
    "\n",
    "## Part1 (Base function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting part of speech tag from nltk.pos_tag to word net compatible format\n",
    "# Simple mapping based on first letter of return tag to make grading consistent\n",
    "# Everything else will be considered noun 'n'\n",
    "posMapping = {\n",
    "# \"First_Letter by nltk.pos_tag\":\"POS_for_lemmatizer\"\n",
    "    \"N\":'n',\n",
    "    \"V\":'v',\n",
    "    \"J\":'a',\n",
    "    \"R\":'r'\n",
    "}\n",
    "\n",
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Standarizing tokens to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 2: Capturing and removing URL using regular expressions.\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "\n",
    "    ######################################################\n",
    "    # Step 3: Removing all punctuations with regards\n",
    "    # to the rules given.\n",
    "\n",
    "    # Remove 's\n",
    "    text = re.sub(r\"'s\", \"\", text)\n",
    "\n",
    "    # Removing apostrophes\n",
    "    text = re.sub(r\"[']\", \"\", text)\n",
    "\n",
    "    # Removing any left punctuation and breaking word by space\n",
    "    text = re.sub(r\"[!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\", \" \", text)\n",
    "    ######################################################\n",
    "    \n",
    "    # Tokenizing the text using nltk.word_tokenize()\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "\n",
    "    # Part-of-speech-tagging every token in tokenized_text\n",
    "    pos_tagged_tokens = nltk.pos_tag(tokenized_text)\n",
    "\n",
    "    # Converting the pos tagged tokens to wordnet format for\n",
    "    # standardization and so they can be used in the lemmatize()\n",
    "    # function.\n",
    "    pos_tagged_tokens = [[tag[0], posMapping.get(tag[1][0], 'n')] for tag in pos_tagged_tokens]\n",
    "    \n",
    "    # Creating a list of lemmas to return through lemmatizer.\n",
    "    list_of_lemmas = []\n",
    "    for pos_tags in pos_tagged_tokens:\n",
    "        try:\n",
    "            list_of_lemmas.append(lemmatizer.lemmatize(pos_tags[0], pos_tags[1]))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "    return list_of_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the function to see if it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(process(\"I'm doing well! How about you?\"))\n",
    "# ['im', 'do', 'well', 'how', 'about', 'you']\n",
    "\n",
    "print(process(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\"))\n",
    "# ['education', 'be', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'lose', 'your', 'temper', 'or', 'your', 'self', 'confidence']\n",
    "\n",
    "print(process(\"been had done languages cities mice\"))\n",
    "# ['be', 'have', 'do', 'language', 'city', 'mice']\n",
    "\n",
    "print(process(\"It's hilarious. Check it out http://t.co/dummyurl\"))\n",
    "# ['it', 'hilarious', 'check', 'it', 'out']\n",
    "\n",
    "print(process(\"See it Sunday morning at 8:30a on RTV6 and our RTV6 app. http:…\"))\n",
    "# ['see', 'it', 'sunday', 'morning', 'at', '8', '30a', 'on', 'rtv6', 'and', 'our', 'rtv6', 'app', 'http', '…']\n",
    "# Here '…' is a special unicode character not in string.punctuation and it is still present in processed text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2(Processing):\n",
    "I will now use the `process()` funciton to convert the pandas df \"tweets_train.csv\". This function ideally should be able to handle any df that contains the column `text`. The df my `process_all()` function should return, replacing every string in `text` with the result of `process()` and retain all other columns as their default values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading df\n",
    "\n",
    "tweets = pd.read_csv(\"tweets_train.csv\", na_filter=False)\n",
    "display(tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" process all text in the dataframe using process() function.\n",
    "    Inputs\n",
    "        df: pd.DataFrame: dataframe containing a column 'text' loaded from the CSV file\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs\n",
    "        pd.DataFrame: dataframe in which the values of text column have been changed from str to list(str),\n",
    "                        the output from process() function. Other columns are unaffected.\n",
    "    \"\"\"\n",
    "\n",
    "    # Converting the series to a list\n",
    "    tweet = list(df[\"text\"])\n",
    "    \n",
    "    # Processing all of the tweets and returning lemmas in a list\n",
    "    # for the tweets\n",
    "    lemmatized_text = [process(text) for text in tweet]\n",
    "\n",
    "    # Putting all of the lemmas in their respective\n",
    "    # places in the text series of the df. \n",
    "    df['text'] = lemmatized_text\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the function to see if it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = process_all(tweets)\n",
    "print(processed_tweets)\n",
    "\n",
    "#       screen_name                                               text\n",
    "# 0             GOP  [rt, gopconvention, oregon, vote, today, that,...\n",
    "# 1    TheDemocrats  [rt, dwstweets, the, choice, for, 2016, be, cl...\n",
    "# 2  HillaryClinton  [trump, call, for, trillion, dollar, tax, cut,...\n",
    "# 3  HillaryClinton  [timkaine, guide, principle, the, belief, that...\n",
    "# 4        timkaine  [glad, the, senate, could, pass, a, thud, milc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Construction\n",
    "\n",
    "The next step is to derive feature vectors from the tokenized tweets. Now I will construct a bag-of-words TF-IDF feature vector. But before that the number of possible words is prohibitively large and not all of them may be useful for our classification task. In order to determine which words to retain and which to omit I will use a common heuristic which is to construct a frequency distribution of words in the corpus and prune out the head and tail of the distribution. The intuition of the above operation is as follows. Very common words (i.e. stopwords) add almost no information regarding similarity of two pieces of text. Similarly with very rare words. NLTK has a list of in-built stop words which is a good substitute for head of the distribution. We will consider a word rare if it occurs only in a single document (row) in whole of `tweets_train.csv`. \n",
    "\n",
    "## Part1 (Feature Matrix Construction):\n",
    "\n",
    "Now I will construct a sparse matrix of features for each tweet with the help of `sklearn.feature_extraction.text.TfidfVectorizer` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)). After some research I figured out that I need to pass a parameter `min_df=2` to filter out the words occuring only in one document in the whole training set. I am going to leave other optional parameters (e.g., `vocab`, `norm`, etc) at their default values. I am going to use parameters like `lowercase` and `tokenizer` to handle `processed_tweets` that is a `list` of tokens (not raw text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_features(processed_tweets, stop_words):\n",
    "    \"\"\" creates the feature matrix using the processed tweet text\n",
    "    Inputs:\n",
    "        processed_tweets: pd.DataFrame: processed tweets read from train/test csv file, containing the column 'text'\n",
    "        stop_words: list(str): stop_words by nltk stopwords (after processing)\n",
    "    Outputs:\n",
    "        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
    "            we need this to tranform test tweets in the same way as train tweets\n",
    "        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Since I am dealing with preprocessed tweets\n",
    "    # I define a tokenizer funtion to just return the \n",
    "    # tokens as it so that it my override the default\n",
    "    # tokenizer and not tokenize them again.\n",
    "    def tokenize(text):\n",
    "        return text\n",
    "    \n",
    "    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(min_df=2, stop_words=stop_words, tokenizer=tokenize, lowercase=False)\n",
    "    \n",
    "    # Fit and transform the processed tweets to create a feature matrix.\n",
    "    vectorizer.fit(processed_tweets['text'])\n",
    "    feature_matrix = vectorizer.transform(processed_tweets['text'])\n",
    "\n",
    "    return vectorizer, feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is recommended to process stopwords according to our data cleaning rules\n",
    "processed_stopwords = list(np.concatenate([process(word) for word in stopwords]))\n",
    "(tfidf, X) = create_features(processed_tweets, processed_stopwords)\n",
    "# Ignore warning\n",
    "tfidf, X\n",
    "\n",
    "# Output (should be similar):\n",
    "# (TfidfVectorizer(lowercase=False, min_df=2,\n",
    "#                  stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
    "#                              'ourselves', 'you', 'youre', 'youve', 'youll',\n",
    "#                              'youd', 'your', 'yours', 'yourself', 'yourselves',\n",
    "#                              'he', 'him', 'his', 'himself', 'she', 'shes', 'her',\n",
    "#                              'hers', 'herself', 'it', 'it', 'it', 'itself', ...],\n",
    "#                  tokenizer=<function create_features.<locals>.<lambda> at 0x2af726660>),\n",
    "#  <17298x8115 sparse matrix of type '<class 'numpy.float64'>'\n",
    "#  \twith 169163 stored elements in Compressed Sparse Row format>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2 (Creation of Labels)\n",
    "For each tweet I assign a class label (0 or 1) using the `screen_name`. 0 for realDonaldTrump, mike_pence, GOP and 1 for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(processed_tweets):\n",
    "    \"\"\" creates the class labels from screen_name\n",
    "    Inputs:\n",
    "        processed_tweets: pd.DataFrame: tweets read from train file, containing the column 'screen_name'\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary numpy array of class labels\n",
    "    \"\"\"\n",
    "    # Creating a boolean array where True corresponds to the republicans\n",
    "    republicans = processed_tweets['screen_name'].isin(['realDonaldTrump', 'mike_pence', 'GOP'])\n",
    "\n",
    "    # Using np.where() to assign 0 or 1 based on condition.\n",
    "    labels = np.where(republicans, 0, 1).astype(int)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = create_labels(processed_tweets)\n",
    "y\n",
    "# 0        0\n",
    "# 1        1\n",
    "# 2        1\n",
    "# 3        1\n",
    "# 4        1\n",
    "#         ..\n",
    "# 17293    0\n",
    "# 17294    0\n",
    "# 17295    0\n",
    "# 17296    1\n",
    "# 17297    0\n",
    "# Name: screen_name, Length: 17298, dtype: int32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification [40%]\n",
    "\n",
    "Now puting things together and learn a model for the classification of tweets. The classifier that I am using is [`sklearn.svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) (Support Vector Machine). \n",
    "\n",
    "At the heart of SVMs is the concept of kernel functions, which determines how the similarity/distance between two data points in computed. `sklearn`'s SVM provides four kernel functions: `linear`, `poly`, `rbf`, `sigmoid` ([details](http://scikit-learn.org/stable/modules/svm.html#svm-kernels)).\n",
    "\n",
    "Through the various functions you implement in this part, you will be able to learn a classifier, score a classifier based on how well it performs, use it for prediction tasks and compare it to a baseline.\n",
    "\n",
    "Through these function I that I implement in this part, I will be able to learn a classifier, score it and based on how well it performs, use it for prediction tasks and compare it to a baseline.\n",
    "\n",
    "Specifically, I will carry out the following tasks in order:\n",
    "\n",
    "1. Implement and evaluate a simple baseline classifier MajorityLabelClassifier.\n",
    "2. Implement the `learn_classifier()` function assuming `kernel` is always one of {`linear`, `poly`, `rbf`, `sigmoid`}. \n",
    "3. Implement the `evaluate_classifier()` function which scores a classifier based on accuracy of a given dataset.\n",
    "4. Implement `best_model_selection()` to perform cross-validation by calling `learn_classifier()` and `evaluate_classifier()` for different folds and determine which of the four kernels performs the best.\n",
    "5. Go back to `learn_classifier()` and fill in the best kernel. \n",
    "\n",
    "## Part1 (Establishing Baseline):\n",
    "\n",
    "To determine whether the classifier is performing well, I will compare it to a baseline classifier. My classifier should beat the baseline in terms of performace measure such as accuracy.\n",
    "\n",
    "In order to establish a baseline I will implement a classifier called `MajorityLabelClassifier` that always predicts the class equal to **mode** of the labels (i.e., the most frequent label) in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MajorityLabelClassifier():\n",
    "    \"\"\"\n",
    "    A classifier that predicts the mode of training labels\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize your parameter here\n",
    "        \"\"\"\n",
    "        self.theta = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Implement fit by taking training data X and their labels y and finding the mode of y\n",
    "        i.e. store your learned parameter\n",
    "        \"\"\"\n",
    "        self.theta = pd.Series(y).mode()[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Implement to give the mode of training labels as a prediction for each data instance in X\n",
    "        return labels\n",
    "        \"\"\"\n",
    "        return np.full(shape=X.shape[0], fill_value=self.theta)\n",
    "\n",
    "\n",
    "\n",
    "baselineClf = MajorityLabelClassifier()\n",
    "baselineClf.fit(X, y)\n",
    "predictions = baselineClf.predict(X)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "print(accuracy)\n",
    "# print(training accuracy) should give 0.5001734304543878"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2 (learn_classifier()):\n",
    "This function assumes kernel is always on of {`linear`, `poly`, `rbf`, `sigmoid`}. Stick to default values for any other optional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_classifier(X_train, y_train, kernel):\n",
    "    \"\"\" learns a classifier from the input features and labels using the kernel function supplied\n",
    "    Inputs:\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features, output of create_features()\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels, output of create_labels()\n",
    "        kernel: str: kernel function to be used with classifier. [linear|poly|rbf|sigmoid]\n",
    "    Outputs:\n",
    "        sklearn.svm.SVC: classifier learnt from data\n",
    "    \"\"\"\n",
    "\n",
    "    clf = sklearn.svm.SVC(kernel=kernel)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = learn_classifier(X, y, 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3 (Evaluate Classifier)\n",
    "The next step now is to evaluate classifier i.e to characterize how good its classification performance is. This is done to select the best model among models, or for the future tune the hyperparameters for the given model.\n",
    "**Cross-validation**:  This is an approach that I will use. To my understanding it divides the data set in $k$ groups (so, called k-fold set). One of teh gropu is used as a test set for evaluation and other groups as training set. The model of hyperparameter with the bast average performance across all k folds is chosen. For this part I will perform 4-fold cross validation to determine the best kernel. I will keep all other hyperparameters default for now. This approach provides robustness toward biasness in validation set. However, it takes more time.\n",
    "\n",
    "**Metric**: I will be using accuracy as my model evaluation metric. The accuracy of the classifier measures the fraction of all dat points taht are correctly classifier by it; it is the ration of number of correct classifications to the total number of (currect and incorrect) classifications. `sklearn.metrics` provides a number of performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, X_validation, y_validation):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "    Inputs:\n",
    "        classifier: sklearn.svm.classes.SVC: classifer to evaluate\n",
    "        X_validation: scipy.sparse.csr.csr_matrix: sparse matrix of features\n",
    "        y_validation: numpy.ndarray(int): dense binary vector of class labels\n",
    "    Outputs:\n",
    "        double: accuracy of classifier on the validation data\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred = classifier.predict(X_validation)\n",
    "\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_validation, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_classifier(classifier, X, y)\n",
    "print(accuracy)\n",
    "# should give around 0.9545612209503989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part4 (Cross Validation)\n",
    "Now I will decide which kernel works best by using cross-validation. The code splits the training data into 4-folds (75% training and 25% validation) by shuffling randomly. For each kernel I will record the average accuracy for all folds and determine the best classifier. Since our dataset is balanced, [`sklearn.model_selection.KFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) can be used for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = sklearn.model_selection.KFold(n_splits=4, random_state=1, shuffle=True)\n",
    "kf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then now i use the following code to determine which classifier is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_selection(kf, X, y):\n",
    "    \"\"\"\n",
    "    Select the kernel giving best results using k-fold cross-validation.\n",
    "    Other parameters should be left default.\n",
    "    Input:\n",
    "    kf (sklearn.model_selection.KFold): kf object defined above\n",
    "    X (scipy.sparse.csr.csr_matrix): training data\n",
    "    y (array(int)): training labels\n",
    "    Return:\n",
    "    best_kernel (string)\n",
    "\n",
    "    # Use the documentation of KFold cross-validation to split ..\n",
    "    # training data and test data from create_features() and create_labels()\n",
    "    # call learn_classifer() using training split of kth fold\n",
    "    # evaluate on the test split of kth fold\n",
    "    # record avg accuracies and determine best model (kernel)\n",
    "    \"\"\"\n",
    "    kernel_accuracy = {}\n",
    "    for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n",
    "        accuracies = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "\n",
    "            # Splitting data into training and testing sets for the \n",
    "            # current fold.\n",
    "            X_train, y_train= X[train_index], y[train_index]\n",
    "            X_test, y_test = X[test_index], y[test_index]\n",
    "            classifier = learn_classifier(X_train, y_train, kernel)\n",
    "            accuracy = evaluate_classifier(classifier, X_test, y_test)\n",
    "\n",
    "            # Appending the accuracy/error to the error array for \n",
    "            # further evaluation of mean error.\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "        # Calculating teh average accuracy for the current kernel \n",
    "        # across all folds\n",
    "        kernel_accuracy[kernel] = np.mean(accuracies)\n",
    "\n",
    "    # Getting the best kernel with the highest average accuracy.\n",
    "    best_kernel = max(kernel_accuracy, key=kernel_accuracy.get)\n",
    "\n",
    "    # Returning the best kernel as a string.\n",
    "    return best_kernel\n",
    "\n",
    "#Test your code\n",
    "best_kernel = best_model_selection(kf, X, y)\n",
    "best_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to write a nice little wrapper function that will use my model to classify unlabeled tweets from tweets_test.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweets(tfidf, classifier, unlabeled_tweets):\n",
    "    \"\"\" predicts class labels for raw tweet text\n",
    "    Inputs:\n",
    "        tfidf: sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used on training data\n",
    "        classifier: sklearn.svm.SVC: classifier learned\n",
    "        unlabeled_tweets: pd.DataFrame: tweets read from tweets_test.csv\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary vector of class labels for unlabeled tweets\n",
    "    \"\"\"\n",
    "\n",
    "    # Processing all the test tweets.\n",
    "    processed_tweets = process_all(unlabeled_tweets)\n",
    "\n",
    "    # Transforming the processed tweets to create a feature matrix\n",
    "    # for the test\n",
    "    X_test = tfidf.transform(processed_tweets['text'])\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I fill in best classifier in the function and re-trian the classifier using all training data\n",
    "classifier = learn_classifier(X, y, 'poly')\n",
    "unlabeled_tweets = pd.read_csv(\"tweets_test.csv\", na_filter=False)\n",
    "y_pred = classify_tweets(tfidf, classifier, unlabeled_tweets)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_classifier(classifier, X, y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier performed better than the baseline training data. This is because we pass in the best type of kernel that we found from the kfold cross validation. When I run the best classifier that I found which is `poly` and calculate it against the ground truth, it givees me an accuracy of 99.7%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
